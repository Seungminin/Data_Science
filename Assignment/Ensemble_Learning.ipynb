{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   class              150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.4 KB\n",
      "\n",
      " None\n",
      "\n",
      "      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     class  \n",
      "145      2  \n",
      "146      2  \n",
      "147      2  \n",
      "148      2  \n",
      "149      2  \n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['class'] = iris.target \n",
    "print(df['class'].unique()) #CLass are [0,1,2] 3 classes 0 : Versicolor, 1 : Setosa, 2 : Virginica \n",
    "print(\"\\n\",df.info()) #sample is 150 entries, feature 4 data, and target 1 data.\n",
    "print(\"\\n\",df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allow duplication of bagging 10 times to pull out data and make 20 samples. \n",
    "#A total of 200 data are created and data that have never been pulled out are stored in Out of data\n",
    "def Bagging(dataset):\n",
    "    bagging_df = pd.DataFrame(columns=dataset.columns)\n",
    "    test_df = pd.DataFrame(columns=dataset.columns)\n",
    "    Out_of_Data = set(range(dataset.shape[0])) #Initial 200 sample(0~199), not duplication \n",
    "\n",
    "    training_index = pd.DataFrame() #save bootstraped index.\n",
    "\n",
    "    #generate 10 bagging dataset and each with 20 records.\n",
    "    for i in range(10):\n",
    "        data_index = list(range(150)) \n",
    "        random_data_index = np.random.choice(data_index, 20, replace=True) #restoration extraction\n",
    "        training_index[f'{i}th'] = random_data_index\n",
    "        bagging_sample = dataset.iloc[random_data_index, :]\n",
    "        bagging_df = pd.concat([bagging_df, bagging_sample], axis=0) #Stored in bagging_df via pulled data indices\n",
    "        Out_of_Data -= set(random_data_index) #remove the sampled data index\n",
    "    \n",
    "    Out_of_Data = list(Out_of_Data)\n",
    "    test_df = dataset.iloc[Out_of_Data, :]\n",
    "    print(\"\\n Not sampled data index : \",Out_of_Data, \"\\n Not sampled data length is : \",len(Out_of_Data))\n",
    "    print(\"\\nThe bootstrap training dataset index\\n\", training_index)\n",
    "    return bagging_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Not sampled data index :  [14, 15, 30, 33, 36, 42, 53, 58, 62, 63, 64, 71, 74, 75, 82, 83, 89, 98, 101, 103, 114, 118, 119, 123, 126, 127, 131, 139, 141, 142] \n",
      " Not sampled data length is :  30\n",
      "\n",
      "The bootstrap training dataset index\n",
      "     0th  1th  2th  3th  4th  5th  6th  7th  8th  9th\n",
      "0    38  132  124   40   72   97   18  144  110    2\n",
      "1    54  144   68   41   34   90   39   26  104    4\n",
      "2   129   59   29  110    5   47  145   24  145   77\n",
      "3   116  109  104   16   65  117   76    0  135  147\n",
      "4    19   67   37   68  137  100   46    2  140   95\n",
      "5    16  111   96   78   49   60   76   79   32   67\n",
      "6    59   87   29   73  110   67  128   10  134   61\n",
      "7   125   26   12   88   61   54   73   61   17    3\n",
      "8    69  129   60   43   66  140   24  100   20   28\n",
      "9    91   84  102  125   48   72   38   94   85   57\n",
      "10   51   23  121   95  140    8    9  113   24  115\n",
      "11  143   28    1   22   50  111   90  100   93    1\n",
      "12   32  144    7   38   27    7   78  124   55  111\n",
      "13   86   40   41   81   31   25   70    2   61   55\n",
      "14   52   46   60  112  100  138  121   35  105   88\n",
      "15  120   19   70   22   48    4   37    4   16  133\n",
      "16   80  102   99    6   11   13   12   92   39  120\n",
      "17  130   94    2  136   85   78   23  140  111   56\n",
      "18  100   92  108  102  146   26  135  149  148  122\n",
      "19   45   31  107    2  111  106   34   44   47   21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksmin\\AppData\\Local\\Temp\\ipykernel_8784\\2636670353.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  bagging_df = pd.concat([bagging_df, bagging_sample], axis=0) #Stored in bagging_df via pulled data indices\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#How to create #testdataset: Find the index as an intersection that has never been selected in the process of creating 10 bootstraps.\n",
    "bagging_df, test_df = Bagging(df)\n",
    "\n",
    "X_train = bagging_df.iloc[:,0:4].values\n",
    "y_train = bagging_df.iloc[:,-1].values\n",
    "\n",
    "X_test = test_df.iloc[:,0:4].values\n",
    "y_test = test_df.iloc[:,-1].values\n",
    "\n",
    "y_train = y_train.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.4 3.  4.5 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.  3.  1.6 0.2]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.  3.  4.8 1.8]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.  3.5 1.3 0.3]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 2 2 0 0 1 0 2 0 0 2 1 0 1 1 2 0 2 1 0 2 0 2 0 0 2 1 1 0 0 2 2 1 2 2\n",
      " 1 0 0 1 1 0 1 1 2 2 2 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 2 1 2 0 0 1 2 0 1 2 0\n",
      " 2 1 2 0 1 2 1 1 2 0 1 2 2 2 0 2 1 0 2 2 1 0 1 0 0 1 1 1 2 2 2 0 0 2 2 2 2\n",
      " 2 1 2 2 1 2 1 0 0 0 2 1 1 0 0 0 1 1 2 1 2 0 2 0 2 2 1 0 2 1 2 1 0 2 0 2 0\n",
      " 1 0 1 1 2 0 1 0 2 0 0 2 2 1 0 1 1 1 1 0 2 0 1 0 0 1 1 2 2 0 2 2 1 0 1 0 1\n",
      " 2 2 2 0 2 0 1 2 0 1 1 0 1 1 0] \n",
      " y_train length is :  200\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] \n",
      " y_test length is :  32\n"
     ]
    }
   ],
   "source": [
    "print(y_train, \"\\n y_train length is : \", len(y_train))\n",
    "print(\"\\n\", y_test, \"\\n y_test length is : \", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_CF(X_train,X_test,y_train,y_test):\n",
    "    bagging_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), \n",
    "    n_estimators=10,\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    "    )\n",
    "    bagging_clf.fit(X_train,y_train)\n",
    "\n",
    "    #initialize each base learners prediction\n",
    "    base_learner_predictions = np.zeros((X_test.shape[0],10))\n",
    "\n",
    "    for i, estimator in enumerate(bagging_clf.estimators_):\n",
    "        base_learner_predictions[:,i] = estimator.predict(X_test)\n",
    "\n",
    "    y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    predictions_df = pd.DataFrame(base_learner_predictions, columns=[f'{i+1}' for i in range(10)])\n",
    "\n",
    "    predictions_df['Ensemble'] = y_pred\n",
    "    predictions_df[\"Reality\"] = y_test\n",
    "\n",
    "    confusion_matrix = Confuse_Matrix(predictions_df['Reality'],predictions_df['Ensemble'])\n",
    "    return predictions_df,confusion_matrix\n",
    "\n",
    "def Confuse_Matrix(reality, ensemble):\n",
    "    confusion_matrix = pd.crosstab(reality,ensemble,rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90625\n",
      "      1    2    3    4    5    6    7    8    9   10  Ensemble  Reality\n",
      "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0         0        0\n",
      "11  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0         1        1\n",
      "12  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0         1        1\n",
      "13  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        1\n",
      "14  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0         1        1\n",
      "15  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0         1        1\n",
      "16  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "17  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "18  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "19  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "20  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "21  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "22  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "23  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "24  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "25  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "26  1.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  2.0         1        2\n",
      "27  1.0  1.0  2.0  1.0  1.0  2.0  2.0  2.0  1.0  2.0         1        2\n",
      "28  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "29  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "30  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n",
      "31  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0  2.0         2        2\n"
     ]
    }
   ],
   "source": [
    "predictions_df,confusion_matrix = bagging_CF(X_train,X_test,y_train,y_test)\n",
    "\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0  1   2\n",
      "Actual              \n",
      "0          11  0   0\n",
      "1           0  4   1\n",
      "2           0  2  14\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
